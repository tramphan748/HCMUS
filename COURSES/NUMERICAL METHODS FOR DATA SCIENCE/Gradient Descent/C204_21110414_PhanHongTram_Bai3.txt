# PhanHongTram_21110414

import numpy as np
import matplotlib.pyplot as plt

def function(w):
    x = w[0]
    y = w[1]
    return (x**2 + y - 7)**2 + (x - y + 1)**2

def grad_function(w):
    x = w[0]
    y = w[1]
    g = np.zeros_like(w)
    g[0] = 4*x**3 + 4*x*y - 26*x - 2*y + 2
    g[1] = 2*x**2 - 2*x + 4*y - 16
    return g

# Numerical gradient dùng để kiểm tra đạo hàm hàm nhiều biến có đúng hay không
def numerical_grad_function(w, function):
    eps = 1e-6
    g = np.zeros_like(w)
    for i in range(len(w)):
        w_pos = w.copy()
        w_neg = w.copy()
        w_pos[i] += eps
        w_neg[i] -= eps
        g[i] = (function(w_pos)-function(w_neg))/(2*eps)
    return g

def check_grad(w, function, grad):
    w = np.random.rand(w.shape[0], w.shape[1])
    gradient = grad_function(w)
    numerical_gradient = numerical_grad_function(w, function)
    if np.linalg.norm(gradient-numerical_gradient) < 1e-4:
        return True
    else:
        return False

w = np.random.randn(2, 1)
print( 'Checking gradient: ', check_grad(w, function, grad_function))


def gradient_descent(learning_rate, w0, N, eps):
    w = [w0]
    i = 0
    for i in range(N):
        w_new = w[-1] - learning_rate*grad_function(w[-1])
        if np.linalg.norm(grad_function(w_new)) < eps:
            print("Found minimum.")
            break
        w.append(w_new)
    else: print(f'The algorithm fails after {N} iterations') 
    return (w, i)

learning_rate = 0.01
w0 = np.array([[2], [2]])
N = 1000
eps = 1e-3

(w1, iter) = gradient_descent(learning_rate, w0, N, eps)

print(f"Solution = {w1[-1].T}, f(x,y) = {function(w1[-1])}, iters = {iter}")


